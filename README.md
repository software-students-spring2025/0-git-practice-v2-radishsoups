# Git Practice
## Bias in AI-based models for medical applications: challenges and mitigation strategies
A link to the article can be found [here](https://www.nature.com/articles/s41746-023-00858-z).

## Article thoughts
This article discusses biases in AI models used for medical purposes and explores possible solutions. Something I found interesting is that bias in medical models is prevalent due to the biases that exist in the training data. The article cites "data collection/preparation, model development, model evaluation, and deployment in clinical settings" as potential sources of bias. This raises a few questions—how can we obtain data suitable to train models? Should we use datasets that were obtained from a broader group, or take from data that was taken from a narrower population? Additionally, can we create software that can detect bias in training data? These are all concerns that are still being addressed, but remain critical as AI takes its place in the medical field in the coming years. 

## Insights by Jasmeen Kaur 
This article is really interesting because it highlights how AI models in medical applications can be biased, leading to serious issues in diagnosis and treatment, especially for underrepresented groups. It explains how bias often comes from imbalanced training data, particularly those lacking diversity, systemic inequalities, and flawed algorithms, making AI less reliable for certain populations. What stood out to me was the focus on mitigation strategies, like improving data diversity, using bias detection techniques, and applying fairness-aware algorithms to make AI more ethical and accurate. I think this is a very crucial topic because AI is becoming a bigger part of healthcare day by day, and ensuring fairness in medical AI could literally save countless lives. Therefore no matter the field, I believe that the key to unlocking AI’s full potential for everyone is diverse and representative datasets. 

